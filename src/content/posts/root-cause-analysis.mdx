---
title: "Lattices, Not Roots: A Systems Theory of Industrial Failure"
date: 2025-08-25
tags: ["essay"]
---

## The Myth of the Root Cause (Singular)

Root Cause Analysis (RCA) has become an article of faith in industrial operations. It satisfies organizational needs for closure, accountability, and regulatory compliance. But it fundamentally misrepresents how complex systems fail.

Modern operations are not linear chains but **lattices** of interdependence. Failures rarely emerge from a single point of breakdown; rather, they arise from the probabilistic alignment of multiple, often mundane conditions across technical, human, and organizational domains.

As such, the “root cause” story is a narrative convenience—a way of compressing high-dimensional complexity into a single explanatory thread. More often than not, it serves to assign responsibility rather than surface truth, which explains the persistent concern that it can be misused as a vehicle for blame[^1]. As Richard Cook observed in *How Complex Systems Fail*, RCA is better understood as a social ritual than a technical discovery.

[^1]: To be clear, I am attacking RCA as it's commonly practiced and institutionally mandated, **not** as it's performed by the top 1% of experts (who make up a sliver of the total RCAs produced or demanded). In talking to folks and reading, I've heard that it's been frequently misused to find a single person or component to blame so the case can be closed and everyone can get back to work.

But the deeper problem with RCA is epistemological: complex systems exhibit **emergent properties** that cannot be reduced to their constituent parts. When we insist on finding "the" cause, we commit what philosophers call the fallacy of misplaced concreteness: treating an analytical abstraction (the singular root) as if it were a physical reality.

**Bottom Line:** The most dangerous failures do not reside in obvious broken components; they hide in the interactions between subsystems, across shifts, and within procedures that organizations fail to see until it is too late.

## RCA as Dimensionality Reduction

Traditional RCA is a **lossy compression algorithm**. It takes a high-dimensional event (involving hundreds of variables evolving over extended time periods) and compresses it into a low-dimensional narrative (typically one or two "root causes"), packaged in a report or short presentation.

Like any compression algorithm, RCA preserves some information while discarding other information.

**What RCA Preserves:**
- Linear causal chains
- Proximate temporal relationships  
- Individual accountability assignments
- Regulatory compliance frameworks

**What RCA Discards:**
- Non-linear interactions
- Correlated patterns across time
- Systemic drift and adaptation
- “Near misses” and latent resilience

**Information Theory Insight:** The information discarded by RCA is precisely the information needed to predict and prevent future failures. By compressing away the interaction patterns, we lose the ability to recognize when similar patterns are forming. This is analogous to the **lossy compression** used in JPEG images—it preserves the information humans consciously notice while discarding information that might be crucial for later machine analysis. These details, lost forever. What then is the ideal?

**Ideal:** Imagine, instead, having access to the RAW footage underlying the JPEG. You can generate a JPEG, PNG, or any other format at any time while the full-resolution source remains intact, and updated on demand.

Applied to industrial operations, this is what a dynamic system analysis enables:

- Continuous ingestion of logs, sensor data, operator notes, and historical events.
- On-demand exploration of the full lattice of interactions across technical, human, procedural, and organizational layers.
- Dynamic RCA: analyses that are not fixed in time but reflect the most up-to-date operational reality, including drift, workarounds, and correlated failure patterns.
- Preservation of high-dimensional, system-level insights that static RCA compresses away, while still producing concise, actionable outputs when needed.

In short: where traditional RCA is a JPEG, dynamic lattice analysis gives engineers access to the RAW footage that is always available, current, and queryable. This approach does not promise perfect prediction but does give the ability to understand, anticipate, and adapt to the complex interactions that drive real-world industrial failures.

## The Information-Theoretic View of System Failure

To understand why lattice thinking is necessary, we must first understand what information theory tells us about complex systems.

In information theory, the complexity of a system is measured not by the number of its parts, but by the number of possible states those parts can occupy collectively, i.e. its *state space*. A system with $n$ binary components has $2^n$ possible states. As $n$ grows, the state space explodes exponentially.

Industrial systems exist in this exponential space. Consider a process unit with 1,000 monitored variables, each capable of being in one of ten discrete states (normal, high-normal, high, high-high, etc.). The total state space contains $10^{1000}$ possible configurations—a number larger than the estimated number of atoms in the observable universe.

Most of these states represent system failure. Only a tiny fraction represent acceptable operation. The fundamental challenge of industrial control is not preventing specific failures, but **navigating safely through an astronomical space of possible system states**.

**Key Insight:** Traditional RCA examines one path through this space—the path that led to the observed failure. But there are countless other paths that could have led to failure, and countless paths that nearly led to failure but were corrected by adaptive responses. Understanding only the realized path tells us little about the structure of the space itself.[^2]

[^2]: This is an important point to emphasize. A reasonable rebuttal is that any incident can be made intractable if we attempt to consider *everything*. Therefore, the best we can do is *mostly manage* things via dimensionality reduction. Given recent technology, we can certainly widen how *much* we can plausibly manage by adopting artificial intelligence and other tools that can reason through contributing factors at a scale and speed far exceeding human operators alone.

## The Statistical Inevitability of Failure

Industrial systems do not fail because of negligence or misdesign alone; they fail because mathematics and the entropy of complex interactions makes failures inevitable.

Consider a system with $K$ critical components, each with reliability $r$ over a given time period. The probability that all components function properly is $r^K$. For large $K$ and $r < 1$, this probability collapses to zero as time extends. Perfect reliability is mathematically impossible.

But this simple model misses the crucial insight: **real systems fail not when components fail independently, but when they fail in correlated patterns**. The corollary to this is that while exhaustive enumeration of all possible failure combinations is infeasible, certain classes of failure patterns occur with disproportionately higher probability. By modeling the system as a network (read: multi-agent system) rather than a set of independent components, we gain the ability to anticipate these patterns more effectively than by focusing on individual components alone.

The mathematics of **percolation theory**[^3] provide a better model. Imagine your industrial system as a network where nodes (components, procedures, people) are connected by edges (dependencies, communications, timing relationships). The system functions when there exists a connected path from inputs to outputs through functioning nodes. System failure occurs when enough nodes fail simultaneously to break all such paths—what network theorists call a **cut set**. Importantly, there are typically many possible cut sets, most of which involve combinations of component states rather than single component failures.

[^3]: See [Wikipedia](https://en.wikipedia.org/wiki/Percolation_theory) for a nice overview of percolation theory: "Assume that some liquid is poured on top of some porous material. Will the liquid be able to make its way from hole to hole and reach the bottom?".

Real operations are even more complex still: these networks are not static but time-varying multi-agent systems. At each of $M$ time steps, the network takes a different configuration, with distinct cut sets that may emerge, evolve, or disappear as operational conditions shift. It is in these dynamics—not the isolated reliability of individual parts—that the true drivers of systemic fragility reside.

**Mathematical Insight:** If your system has redundancy factor $R$ (meaning $R$ independent paths must be severed to cause failure), and each path has $n$ nodes with individual failure probability $p$, then the probability of system failure is not simply related to $p$, but depends on the complex combinatorics of how paths can be severed simultaneously.

**Bottom Line:** This is one way in which simple reliability calculations can mislead. They assume independence when real systems exhibit complex interdependencies.

## The Thermodynamics of Operational Drift

To understand how systems drift toward failure, we can borrow concepts from thermodynamics and statistical mechanics.

Every operational procedure (SOPs, playbooks), equipment specification, and organizational structure represents a **constraint** that *presumably* limits the system's degrees of freedom. These constraints require energy to maintain—in the form of training, maintenance, monitoring, and enforcement.

But systems naturally tend toward **maximum entropy**—the state of maximum disorder consistent with their constraints. When maintenance energy decreases (due to resource or attention pressure), constraints slacken and the system wanders new regions of its state space.

This is **operational drift**: the gradual migration of system behavior away from design specifications and toward configurations that minimize local effort while maintaining *somewhat* functional output. This should be unsurprising as it affects all systems, especially large ones.

**Thermodynamic Insight:** Drift is not a failure of human discipline—it is a fundamental physical process. Systems will drift unless energy is continuously invested to maintain constraints. The question is not whether drift will occur, but whether it will drift toward regions of greater or lesser resilience.

**Practical Implication:** The most important metric in industrial operations is not uptime or efficiency, but *constraint maintenance energy* or **adherence effort** as I prefer to call it—how much effort is required to keep the system operating within its designed envelope.

## The Architecture of Emergent Failure

Modern failures are rarely about single components—they are about the **phase transitions** that occur when multiple system parameters cross critical thresholds simultaneously. The applied maintenance effort falls below the needed **adherence effort** of the operation, after which the operation "stumbles" (probabilistically) into a failure mode, *eventually*.

In physics, phase transitions occur when small changes in control parameters (like temperature or pressure) cause dramatic changes in system behavior (like ice melting or a magnet losing its magnetization). The same mathematics applies to industrial systems.

Consider these examples of industrial "phase transitions":

* **Cognitive load transition**: An operator managing routine operations transitions to being overwhelmed when alarm frequency crosses a critical threshold, causing response time to increase nonlinearly.

* **Maintenance backlog transition**: A maintenance organization functioning normally suddenly cannot keep pace when the rate of new work orders crosses a critical value relative to crew capacity.

* **Communication bandwidth transition**: Information flow between shifts or departments functions adequately until complexity crosses a threshold where informal channels become saturated.

These transitions can be **non-linear**. Small changes in underlying parameters can cause sudden, dramatic changes in system behavior. This is why traditional linear thinking ("a little more stress causes a little more problems") fails to predict system breakdowns. Near these critical points, systems exhibit increased susceptibility—small perturbations that would normally be absorbed can trigger large-scale reorganization. Most industrial accidents occur when multiple subsystems are simultaneously near their critical points.

## The Paradox of Functional Resilience

Every critical system exhibits what we might call *functional resilience*—the ability to maintain output despite internal degradation. Some of these adaptations are systematic; some are ad hoc, unrecorded, and eventually forgotten. Industrial operations therefore run on an invisible economy of adaptations, including:

- **Maintenance workarounds:** Field modifications that keep equipment running
- **Procedural accommodations:** Informal practices that handle edge cases
- **Supply chain redundancies:** Multiple suppliers, safety stocks, expedited options
- **Production quality containment:** Extra inspections, rework stations, or bypassing non-critical sensors
- **Direct operator intervention:** Hand adjustment of feed rates, process temperature, and other process parameters to compensate for idiosyncratic equipment/plant behavior.

We can formalize this intuition with a simple relation.
$$
\text{applied effort} + \text{adaptations} \geq \text{adherence effort}
$$

*Applied effort* is planned, recorded labor and resources; *adaptations* are the unrecorded workarounds and compensations that operators use to maintain output. *Adherence effort* is the effort required to operate strictly according to design specifications.

When the sum of applied effort and adaptations falls below adherence effort, the system becomes vulnerable. Crucially, because many adaptations are volatile and context-dependent, traditional metrics may overestimate robustness, masking hidden fragility. A technician leaves, firmware changes, or a temporary workaround disappears, and the apparent resilience vanishes.

**The Adaptation Paradox:**
- Systems that cannot adapt fail quickly and obviously
- Systems that adapt too well fail slowly and invisibly
- Optimal systems adapt just enough to handle normal variation while maintaining structural integrity

**Practical Insight:** The goal is neither to eliminate adaptation (impossible) nor to allow uncontrolled adaptation (dangerous). Modern operational tools can act like a reliability engineer you can consult on demand, capturing and surfacing effective adaptations as they emerge, while highlighting which are no longer relevant. Rather than codifying these insights into rigid SOPs, they are made contextually accessible and queryable, enabling teams to channel adaptation deliberately. This preserves flexibility, reduces latent fragility, and strengthens systemic resilience—essentially giving operations a continuously updated, expert perspective on the lattice of system behaviors.


## Why Lattice Thinking Is Theoretically Superior

The shift from root cause to lattice thinking is not merely methodological—it reflects a deeper understanding of **complex adaptive systems**.

**Reductionism vs. Systems Thinking:**
- RCA embodies **reductionist** thinking: complex phenomena can be understood by breaking them down into simple components
- Lattice thinking embodies **systems** thinking: complex phenomena have **emergent properties** that cannot be understood from components alone

**Linear vs. Non-linear Causation:**
- RCA assumes **linear causation**: effects are proportional to causes
- Lattice thinking recognizes **non-linear causation**: small causes can have large effects, and large causes can have small effects, depending on system state

**Static vs. Dynamic Analysis:**
- RCA provides a **static snapshot** of the failure moment
- Lattice thinking analyzes **dynamic trajectories** through system state space over time

**Deterministic vs. Probabilistic Reasoning:**
- RCA seeks **deterministic explanations**: what happened and why it was inevitable
- Lattice thinking uses **probabilistic reasoning**: what were the likelihood patterns and how do they suggest future vulnerabilities

**Theoretical Foundation:** These differences reflect fundamental advances in our understanding of complex systems that have occurred since traditional RCA methodologies were developed. Lattice thinking incorporates insights from network theory, information theory, complexity science, and non-linear dynamics.


## Implementation Strategy: From Root Cause to Lattice Analysis

**Phase 1: Augmented RCA**

* Maintain traditional RCA for regulatory compliance and reporting
* Incorporate network mapping to capture **interaction pathways** that contributed to the incident
* Document **near misses and successful adaptations**, preserving the system’s latent resilience for future reference

**Phase 2: Probabilistic Cut Set Analysis**

* Model your top operational scenarios as **dynamic networks**
* Identify the most probable **cut sets**—combinations of failures or degraded elements that could trigger systemic issues
* Develop monitoring and alerting mechanisms to flag when multiple elements of a cut set are simultaneously stressed

**Phase 3: Dynamic Lattice Awareness**

* Implement **real-time state space monitoring** to track the system’s position within its lattice of interactions
* Provide operators and engineers with a **consultable lattice perspective**, akin to an on-demand reliability expert, that highlights emergent patterns, adaptation status, and potential cut sets
* Train teams to interpret these lattice-level signals and respond proactively, rather than reacting to isolated component failures

**Phase 4: Lattice-Informed System Design**

* Redesign processes and systems with explicit attention to **network topology** and interdependencies
* Optimize for **graceful degradation** rather than perfect prevention
* Build **adaptive capacity** as a designed system property, ensuring the organization can absorb disturbances while maintaining functional output

## Conclusion: The Mathematics of Resilience

The root cause paradigm persists because it offers cognitive closure in an uncertain world. But closure comes at the cost of understanding. By insisting on simple explanations for complex phenomena, we blind ourselves to the mathematical structures that actually govern system behavior.

Lattice thinking requires intellectual courage—the willingness to embrace probabilistic reasoning, non-linear dynamics, and emergent properties. It demands that we think in networks rather than chains, in state spaces rather than event sequences, in adaptation rather than prevention.

But this intellectual investment pays dividends. Organizations that understand their operational lattices or invest Lattice operational tools and platforms can:
- **Predict vulnerabilities** before they manifest as failures
- **Design resilience** as a fundamental system property  
- **Adapt faster** than complexity can overwhelm them
- **Learn systematically** from both failures and near-misses

The root cause is dead. Long live the **Lattice**.
